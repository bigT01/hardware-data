# -*- coding: utf-8 -*-
"""DatAnalit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RYqmNBSlWGw2cHGPQ0jkIywgP-k8BRww
"""

import json
from collections import Counter

with open("/content/tematiche_sedi.json", "r") as f:
    data = json.load(f)

nodes = data['nodi']

# Collect livello1 data
liv1_counts = Counter()
# Collect livello2 data
liv2_counts = Counter()

for node in nodes:
    for l1 in node.get('livello1', []):
        liv1_counts[l1] += 1
    for l2 in node.get('livello2', []):
        liv2_counts[l2] += 1

liv1_counts, liv2_counts

import matplotlib.pyplot as plt

# We'll create bar charts of the counts.
fig1, ax1 = plt.subplots()
ax1.bar(liv1_counts.keys(), liv1_counts.values())
ax1.set_title("Distribuzione delle macro-categorie (livello1)")
plt.xticks(rotation=90)
plt.show()

fig2, ax2 = plt.subplots()
# we'll only take the top 10 for livello2, or it's too big
common_liv2 = liv2_counts.most_common(10)
keys_l2, values_l2 = zip(*common_liv2)
ax2.bar(keys_l2, values_l2)
ax2.set_title("Le 10 competenze (livello2) più frequenti")
plt.xticks(rotation=90)
plt.show()

"""HOMEWORK1: FIRST TASK  .--> ANALYSIS SHOULD BE TOLERANT TO CASE SENSITIVENESS

#Geographical Organization
"""

region_map = {
    "Aquila": "Abruzzo",
    "Bari": "Puglia",
    "Benevento": "Campania",
    "Bologna": "Emilia-Romagna",
    "Catania": "Sicilia",
    "Firenze": "Toscana",
    "Messina": "Sicilia",
    "Milano (UNIMI)": "Lombardia",
    "Milano (POLIMI)": "Lombardia",
    "Modena": "Emilia-Romagna",
    "Napoli (FEDERICO II)": "Campania",
    "Napoli (uniparthenope)": "Campania",
    "Padova": "Veneto",
    "Parma": "Emilia-Romagna",
    "Pavia": "Lombardia",
    "Pisa": "Toscana",  # Qui ci sono due "Pisa" nel JSON, le gestiamo insieme
    "Roma": "Lazio",
    "Salerno": "Campania",
    "Siena": "Toscana",
    "Torino": "Piemonte",  # Anche qui appaiono due voci "Torino", le gestiamo insieme
    "Trento": "Trentino-Alto Adige",
    "Verona": "Veneto"
}

from collections import defaultdict

region_livello1 = defaultdict(list)
region_livello2 = defaultdict(list)
region_count = defaultdict(int)

for node in data["nodi"]:
    sede = node["nodo"]#bari, milano....
    regione = region_map[sede]
    region_count[regione] += 1

    #collecting information to livello1 and livello2
    for l1 in node.get("livello1", []):
        region_livello1[regione].append(l1)
    for l2 in node.get("livello2", []):
        region_livello2[regione].append(l2)

#region_counter contains the number of nodes for each region
#region_livel1 a list af all livel 1 entrie  -- lev2 list of level2 entries

"""#Count and Display the result"""

# Esempio: contiamo quante volte appare ogni macro-categoria a livello regionale
#count homany times each macro catergories appear
from collections import Counter

for regione in region_count:
    print(f"Regione: {regione}, sedi = {region_count[regione]}")
    print("  Livello1 più frequenti:")
    c = Counter(region_livello1[regione])
    for cat, freq in c.most_common():
        print(f"    {cat}: {freq} volta/e")
    print()  #we print each item and its count

"""#HOMEWORK 1  TODO ---> CHANGE AND ENLARGE THE SIZE OF THE LIST OF LEVEL 1 FOR EACH REGION

#NODE SELECTION
"""

selected_nodes_for_IoT = []

for node in data["nodi"]:
    competenze2 = " ".join(node.get("livello2", []))  # Unisci testo livello2 in un’unica stringa
    if "IoT" in competenze2 or "IoT" in " ".join(node.get("livello1", [])):
        selected_nodes_for_IoT.append(node["nodo"]) #ADDING THE NODE with IOT to the list
#compentece in iot
print("Sedi con competenze IoT:")
for sede in selected_nodes_for_IoT:
    print(" -", sede)

ve to read

"""TODO3 --- SELECT OTHER TOPIC! NOT ONLY IOT! pls consider other 4 most cited topic, by using the mecanism of substring --> you have to read the level 1 and level 2 topics and you have to extrapolate the main topic: for eample IOT edge computing and IOT arhitectures should be summarized in one IOT topic"""

# prompt: generate what are the competences occurring togheter, pls generate for me a cooccurrences matrix over the competences

from collections import defaultdict

cooccurrence_matrix = defaultdict(lambda: defaultdict(int))

for node in nodes:
    competences = node.get('livello2', [])
    for i in range(len(competences)):
        for j in range(i + 1, len(competences)):
            comp1 = competences[i]
            comp2 = competences[j]
            cooccurrence_matrix[comp1][comp2] += 1
            cooccurrence_matrix[comp2][comp1] += 1

# Print the cooccurrence matrix
for comp1, related_comps in cooccurrence_matrix.items():
    print(f"Competence: {comp1}")
    for comp2, count in related_comps.items():
        print(f"  - {comp2}: {count}")
    print()

# prompt: pls plot the cooccorrences matrix

import matplotlib.pyplot as plt
import numpy as np

# Assuming cooccurrence_matrix is already created as in your previous code

# Extract data for the plot
competences = list(cooccurrence_matrix.keys())
matrix_data = []
for comp1 in competences:
    row = []
    for comp2 in competences:
        row.append(cooccurrence_matrix[comp1][comp2])
    matrix_data.append(row)

# Create the plot
fig, ax = plt.subplots(figsize=(10, 8))  # Adjust figure size as needed
im = ax.imshow(matrix_data, cmap='viridis')  # Use a suitable colormap

# Set ticks and labels
ax.set_xticks(np.arange(len(competences)))
ax.set_yticks(np.arange(len(competences)))
ax.set_xticklabels(competences, rotation=90)
ax.set_yticklabels(competences)

# Add colorbar
cbar = ax.figure.colorbar(im, ax=ax)
cbar.ax.set_ylabel("Cooccurrence Count", rotation=-90, va="bottom")

# Add annotations (cooccurrence counts) to the heatmap
for i in range(len(competences)):
    for j in range(len(competences)):
        text = ax.text(j, i, matrix_data[i][j], ha="center", va="center", color="w")

ax.set_title("Cooccurrence Matrix of Competences")
fig.tight_layout()
plt.show()

```python
# Import the necessary libraries
import streamlit as st        # Streamlit is used to quickly build interactive web 
                                #apps in Python
import json                   # The json library is used to load the JSON file and work with JSON data
from collections import defaultdict, Counter
# 'defaultdict' is a dictionary-like object which doesn't raise a KeyError if the key is missing (it provides a default).
# 'Counter' is used to count hashable objects; often handy for frequency counts.

# ----------------------------------
# 1) Load the JSON data from file
# ----------------------------------
with open("tematiche_sedi.json", "r") as f:
    # 'data' will be a Python dictionary containing the JSON structure
    data = json.load(f)

# ----------------------------------
# 2) Set the Streamlit page title
# ----------------------------------
st.title("Mappatura competenze del Lab CINI - Embedded Systems & Smart Manufacturing")
# This title will appear at the top of our Streamlit web app

# ----------------------------------
# 3) Create a text input for keyword-based search
# ----------------------------------
# Allows the user to type a keyword (e.g., "IoT", "FPGA", "Real-time", etc.)
keyword = st.text_input("Cerca una parola chiave (es. IoT, FPGA, Real-time, etc.):")

#TODO-> SELECT 6 KEYWORDS FROM THE JSON FILE
# ----------------------------------
# 4) Filter logic
# ----------------------------------
# We'll collect all relevant 'sedi' (nodes) that contain the given keyword
# either in 'livello1' or 'livello2'
filtered_sedi = []

# Check if the user entered a keyword
if keyword:
    # Iterate through each node in the JSON data
    for node in data["nodi"]:
        # Combine the text of livello1 and livello2 into a single string
        # ".join()" merges all items in the list with a space in between
        all_text = " ".join(node.get("livello1", [])) + " " + " ".join(node.get("livello2", []))

        # We convert both 'keyword' and 'all_text' to lowercase to ensure case-insensitive matching
        if keyword.lower() in all_text.lower():
            # If the keyword is found, append the name of the node (sede) to the list
            filtered_sedi.append(node["nodo"])

# ----------------------------------
# 5) Display the filtered results
# ----------------------------------
# If we've found any 'sedi' that match the keyword, show them
if filtered_sedi:
    st.write(f"Sedi che contengono la keyword '{keyword}':")
    for s in filtered_sedi:
        st.write("-", s)
# Otherwise, let the user know no match was found
else:
    st.write("Nessuna sede corrisponde alla ricerca oppure non hai inserito una keyword.")

# ----------------------------------
# 6) Display a table of all sedi and their competenze (livello1, livello2)
# ----------------------------------
st.write("## Elenco sedi e competenze principali (livello1, livello2)")

# For each node (representing a 'sede'), we print out:
#   - The name of the sede
#   - The comma-separated list of livello1 competenze
#   - The comma-separated list of livello2 competenze
for node in data["nodi"]:
    # st.write() prints text on the Streamlit app
    st.write(f"**{node['nodo']}**")  # Bold the name of the sede
    # Display all livello1 items
    st.write(" - Livello1:", ", ".join(node.get("livello1", [])))
    # Display all livello2 items
    st.write(" - Livello2:", ", ".join(node.get("livello2", [])))
    st.write("---")  # A simple separator (horizontal rule) after each sede
```